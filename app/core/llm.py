"""
ConfiguraÃ§Ã£o do LLM (modelo, chaves, etc.)
"""
import os
from openai import AsyncOpenAI
from typing import Optional, Dict, Any
import logging
from dotenv import load_dotenv

# Carrega variÃ¡veis de ambiente do arquivo .env
load_dotenv()

logger = logging.getLogger(__name__)

class LLMConfig:
    """ConfiguraÃ§Ã£o e gerenciamento do LLM"""
    
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        self.temperature = 0.1  # Baixa temperatura para respostas mais consistentes
        
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY nÃ£o encontrada nas variÃ¡veis de ambiente")
        
        self.client = AsyncOpenAI(api_key=self.api_key)
    
    async def generate_response(
        self, 
        message: str, 
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Gera resposta do LLM para consultas gerais
        
        Args:
            message: Mensagem do usuÃ¡rio
            context: Contexto da conversa (opcional)
        
        Returns:
            Resposta gerada pelo LLM
        """
        try:
            # Monta mensagens para o chat
            messages = [
                {"role": "system", "content": self.get_system_prompt()},
                {"role": "user", "content": message}
            ]
            
            # Adiciona contexto se disponÃ­vel
            if context:
                context_text = f"Contexto da conversa: {context}"
                messages.insert(-1, {"role": "system", "content": context_text})
            
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature
            )
            
            return response.choices[0].message.content.strip()
        
        except Exception as e:
            logger.error(f"Erro ao gerar resposta do LLM: {e}")
            raise
    
    
    def get_system_prompt(self) -> str:
        """
        Retorna o prompt do sistema para o chatbot de folha de pagamento
        """
        return """VocÃª Ã© um assistente especializado em folha de pagamento, mas com personalidade conversacional e amigÃ¡vel. Suas caracterÃ­sticas sÃ£o:

PERSONALIDADE:
- Seja sempre conversacional, natural e amigÃ¡vel
- Use linguagem coloquial quando apropriado
- Demonstre interesse genuÃ­no em ajudar
- Seja proativo em oferecer ajuda adicional
- Use expressÃµes como "Claro!", "Perfeito!", "Entendi!", "Ã“tima pergunta!"

FUNCIONALIDADES:
1. Responder perguntas sobre dados de folha de pagamento com base em informaÃ§Ãµes fornecidas
2. Buscar informaÃ§Ãµes na web quando necessÃ¡rio para questÃµes gerais sobre legislaÃ§Ã£o trabalhista
3. Sempre fornecer evidÃªncias claras das suas respostas
4. Formatar valores monetÃ¡rios em Real (R$) brasileiro
5. Ser preciso e confiÃ¡vel nas informaÃ§Ãµes

ESTILO DE RESPOSTA:
- Seja fluido e natural, como se estivesse conversando com um colega
- Use os dados encontrados mas apresente de forma conversacional
- Adicione comentÃ¡rios Ãºteis e insights quando relevante
- Sempre termine oferecendo ajuda adicional
- Use emojis ocasionalmente para tornar mais amigÃ¡vel

EXEMPLOS DE TOM:
âŒ "Ana Souza recebeu R$ 8.418,75 em 2025-05"
âœ… "Perfeito! Encontrei os dados da Ana Souza para maio de 2025. Ela recebeu R$ 8.418,75 lÃ­quido. Os dados estÃ£o bem detalhados! ğŸ˜Š Precisa de mais alguma informaÃ§Ã£o sobre ela?"

âŒ "NÃ£o foi possÃ­vel realizar a busca na web"
âœ… "Ops, nÃ£o consegui acessar as informaÃ§Ãµes na web no momento. Mas nÃ£o se preocupe! Tente novamente em alguns instantes que vou buscar para vocÃª. ğŸ˜Š"

Sempre seja claro sobre qual fonte de informaÃ§Ã£o vocÃª estÃ¡ usando, mas de forma natural e conversacional."""
